#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandasai as pai
import os
import re
import datetime
import jieba
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# é…ç½®APIå¯†é’¥
pai.api_key.set("PAI-8cf934ae-f430-4325-9f7c-ec80cc2f7c88")

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("exports/nlp_charts", exist_ok=True)

# åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼šç¤¾äº¤åª’ä½“è¯„è®º
def create_social_media_data(n_samples=500):
    """åˆ›å»ºåŒ…å«ç¤¾äº¤åª’ä½“è¯„è®ºçš„æ¨¡æ‹Ÿæ•°æ®é›†"""
    
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
    np.random.seed(42)
    
    # è¯„è®ºä¸»é¢˜
    topics = ["æ‰‹æœº", "ç”µè„‘", "å¹³æ¿", "æ™ºèƒ½æ‰‹è¡¨", "è€³æœº", "æ™ºèƒ½å®¶å±…", "ç›¸æœº", "æ¸¸æˆæœº"]
    
    # å“ç‰Œ
    brands = {
        "æ‰‹æœº": ["è‹¹æœ", "åä¸º", "å°ç±³", "ä¸‰æ˜Ÿ", "OPPO", "vivo"],
        "ç”µè„‘": ["è”æƒ³", "æˆ´å°”", "æƒ æ™®", "åç¡•", "è‹¹æœ", "å¾®è½¯"],
        "å¹³æ¿": ["è‹¹æœ", "åä¸º", "å°ç±³", "ä¸‰æ˜Ÿ", "è”æƒ³"],
        "æ™ºèƒ½æ‰‹è¡¨": ["è‹¹æœ", "åä¸º", "å°ç±³", "ä¸‰æ˜Ÿ", "OPPO"],
        "è€³æœº": ["è‹¹æœ", "åä¸º", "å°ç±³", "ç´¢å°¼", "Bose", "æ£®æµ·å¡å°”"],
        "æ™ºèƒ½å®¶å±…": ["å°ç±³", "åä¸º", "é˜¿é‡Œ", "æµ·å°”", "ç¾çš„"],
        "ç›¸æœº": ["ä½³èƒ½", "å°¼åº·", "ç´¢å°¼", "å¯Œå£«", "å¾•å¡"],
        "æ¸¸æˆæœº": ["ç´¢å°¼", "å¾®è½¯", "ä»»å¤©å ‚"]
    }
    
    # åˆ›å»ºå¸¸è§çš„æ­£é¢ã€ä¸­æ€§ã€è´Ÿé¢è¯„è®ºæ¨¡æ¿
    positive_templates = [
        "æˆ‘å¾ˆå–œæ¬¢è¿™æ¬¾{brand}çš„{topic}ï¼Œç”¨èµ·æ¥éå¸¸èˆ’é€‚ã€‚",
        "åˆšä¹°äº†{brand}çš„æ–°{topic}ï¼ŒçœŸçš„å¤ªå¥½ç”¨äº†ï¼",
        "{brand}çš„{topic}è´¨é‡å¾ˆå¥½ï¼Œå¾ˆè€ç”¨ï¼Œæ¨èè´­ä¹°ã€‚",
        "æˆ‘ç”¨äº†{brand}çš„{topic}å·²ç»ä¸¤å¹´äº†ï¼Œä¸€ç›´å¾ˆç¨³å®šï¼Œæ²¡æœ‰ä»»ä½•é—®é¢˜ã€‚",
        "{brand}è¿™æ¬¾{topic}çš„æ€§ä»·æ¯”çœŸçš„å¾ˆé«˜ï¼Œç‰©è¶…æ‰€å€¼ã€‚",
        "{topic}ç•Œçš„æ ‡æ†ï¼Œ{brand}å®è‡³åå½’ã€‚",
        "è®¾è®¡ç²¾ç¾ï¼Œåšå·¥ç²¾è‰¯ï¼Œ{brand}çš„{topic}æ°¸è¿œè®©äººæ»¡æ„ã€‚",
        "ä¹°äº†{brand}çš„{topic}ï¼Œé¢œå€¼å’Œæ€§èƒ½éƒ½å¾ˆæ»¡æ„ï¼Œèµä¸€ä¸ªï¼",
        "{brand}çš„å®¢æœå¾ˆå¥½ï¼Œ{topic}æœ‰å°é—®é¢˜é©¬ä¸Šå°±è§£å†³äº†ã€‚",
        "è¿™æ˜¯æˆ‘ç”¨è¿‡æœ€å¥½çš„{topic}ï¼Œ{brand}æ²¡æœ‰è®©æˆ‘å¤±æœ›ã€‚"
    ]
    
    neutral_templates = [
        "{brand}çš„{topic}è¿˜å¯ä»¥ï¼Œä½†ä»·æ ¼æœ‰ç‚¹è´µã€‚",
        "åˆšå…¥æ‰‹{brand}çš„{topic}ï¼Œæ„Ÿè§‰ä¸€èˆ¬èˆ¬å§ã€‚",
        "è¿™æ¬¾{brand}{topic}æœ‰ä¼˜ç‚¹ä¹Ÿæœ‰ç¼ºç‚¹ï¼Œæ€»ä½“æ¥è¯´ä¸­è§„ä¸­çŸ©ã€‚",
        "{brand}çš„{topic}å¤–è§‚ä¸é”™ï¼Œä½†åŠŸèƒ½ä¸Šæ²¡ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ã€‚",
        "ç”¨äº†ä¸€å‘¨{brand}çš„{topic}ï¼Œæ„Ÿè§‰è¿˜è¡Œï¼Œä¸ç®—ç‰¹åˆ«æƒŠè‰³ã€‚",
        "{brand}è¿™æ¬¾{topic}è´¨é‡è¿˜è¡Œï¼Œä½†åˆ›æ–°ä¸è¶³ã€‚",
        "æ¯”èµ·ä¸Šä¸€ä»£ï¼Œ{brand}è¿™æ¬¾æ–°{topic}æ”¹è¿›ä¸å¤§ã€‚",
        "{topic}æœ¬èº«æ²¡é—®é¢˜ï¼Œå°±æ˜¯{brand}çš„å”®åæœåŠ¡ä¸€èˆ¬ã€‚",
        "ä¹°{brand}çš„{topic}ä¸»è¦æ˜¯å†²ç€å“ç‰Œå»çš„ï¼Œäº§å“æœ¬èº«ä¸­è§„ä¸­çŸ©ã€‚",
        "{brand}çš„{topic}å’Œç«å“ç›¸æ¯”å„æœ‰åƒç§‹ã€‚"
    ]
    
    negative_templates = [
        "ä¸æ¨è{brand}çš„è¿™æ¬¾{topic}ï¼Œè´¨é‡å¤ªå·®äº†ã€‚",
        "ä¹°äº†{brand}çš„{topic}åæ‚”æ­»äº†ï¼Œç³»ç»Ÿç»å¸¸å´©æºƒã€‚",
        "{brand}çš„{topic}ä»·æ ¼é‚£ä¹ˆè´µï¼Œä½†ä½“éªŒå´å¾ˆå·®ã€‚",
        "ç”¨äº†ä¸€ä¸ªæœˆï¼Œ{brand}çš„{topic}å°±å‡ºé—®é¢˜äº†ï¼Œå¤ªå¤±æœ›äº†ã€‚",
        "{brand}çš„{topic}å¤–è§‚ä¸é”™ï¼Œä½†ç”µæ± ç»­èˆªå¤ªå·®ã€‚",
        "åƒä¸‡åˆ«ä¹°{brand}çš„{topic}ï¼Œæµªè´¹é’±ï¼",
        "{brand}çš„å”®åæœåŠ¡å¤ªå·®äº†ï¼Œ{topic}åäº†ç»´ä¿®å¾ˆéº»çƒ¦ã€‚",
        "åŒä»·ä½ä¸‹ï¼Œ{brand}çš„{topic}æ˜¯æˆ‘ç”¨è¿‡æœ€å·®çš„ä¸€æ¬¾ã€‚",
        "{brand}çš„{topic}å‘çƒ­ä¸¥é‡ï¼Œä½“éªŒå¾ˆå·®ã€‚",
        "å¯¹{brand}è¿™æ¬¾{topic}å®Œå…¨å¤±æœ›ï¼Œå°†æ¥å†ä¹Ÿä¸ä¼šé€‰æ‹©è¿™ä¸ªå“ç‰Œäº†ã€‚"
    ]
    
    # åˆ›å»ºæ—¥æœŸèŒƒå›´ï¼šè¿‡å»ä¸€å¹´
    end_date = datetime.datetime.now()
    start_date = end_date - datetime.timedelta(days=365)
    date_range = (end_date - start_date).days
    
    # åˆ›å»ºæ•°æ®
    data = {
        "ID": range(1, n_samples + 1),
        "æ—¥æœŸ": [],
        "ä¸»é¢˜": [],
        "å“ç‰Œ": [],
        "è¯„è®º": [],
        "æƒ…æ„Ÿ": [],
        "æƒ…æ„Ÿåˆ†æ•°": [],
        "ç‚¹èµæ•°": [],
        "è¯„è®ºé•¿åº¦": [],
        "æ˜¯å¦åŒ…å«è¡¨æƒ…": [],
        "å¹³å°": []
    }
    
    platforms = ["å¾®åš", "çŸ¥ä¹", "å°çº¢ä¹¦", "æŠ–éŸ³", "Bç«™", "å¾®ä¿¡"]
    
    for i in range(n_samples):
        # éšæœºé€‰æ‹©æ—¥æœŸ
        random_days = np.random.randint(0, date_range)
        post_date = start_date + datetime.timedelta(days=random_days)
        data["æ—¥æœŸ"].append(post_date.strftime("%Y-%m-%d"))
        
        # éšæœºé€‰æ‹©ä¸»é¢˜å’Œå“ç‰Œ
        topic = np.random.choice(topics)
        brand = np.random.choice(brands[topic])
        data["ä¸»é¢˜"].append(topic)
        data["å“ç‰Œ"].append(brand)
        
        # éšæœºé€‰æ‹©æƒ…æ„Ÿå€¾å‘
        sentiment_probs = [0.6, 0.25, 0.15]  # æ­£é¢, ä¸­æ€§, è´Ÿé¢çš„æ¦‚ç‡
        sentiment = np.random.choice(["æ­£é¢", "ä¸­æ€§", "è´Ÿé¢"], p=sentiment_probs)
        data["æƒ…æ„Ÿ"].append(sentiment)
        
        # åŸºäºæƒ…æ„Ÿé€‰æ‹©è¯„è®ºæ¨¡æ¿
        if sentiment == "æ­£é¢":
            template = np.random.choice(positive_templates)
            sentiment_score = round(np.random.uniform(0.6, 1.0), 2)
        elif sentiment == "ä¸­æ€§":
            template = np.random.choice(neutral_templates)
            sentiment_score = round(np.random.uniform(0.4, 0.6), 2)
        else:
            template = np.random.choice(negative_templates)
            sentiment_score = round(np.random.uniform(0.0, 0.4), 2)
        
        # å¡«å……æ¨¡æ¿
        comment = template.format(brand=brand, topic=topic)
        
        # éšæœºæ·»åŠ ä¸€äº›è¡¨æƒ…ç¬¦å·
        has_emoji = np.random.choice([True, False], p=[0.3, 0.7])
        if has_emoji:
            emojis = ["ğŸ˜Š", "ğŸ‘", "â¤ï¸", "ğŸ˜¢", "ğŸ˜¡", "ğŸ¤”", "ğŸ‘", "ğŸ‰", "ğŸ”¥", "ğŸ‘"]
            # åœ¨è¯„è®ºæœ«å°¾æ·»åŠ 1-3ä¸ªéšæœºè¡¨æƒ…
            n_emojis = np.random.randint(1, 4)
            for _ in range(n_emojis):
                comment += np.random.choice(emojis)
        
        data["è¯„è®º"].append(comment)
        data["æƒ…æ„Ÿåˆ†æ•°"].append(sentiment_score)
        data["ç‚¹èµæ•°"].append(np.random.randint(0, 1000))
        data["è¯„è®ºé•¿åº¦"].append(len(comment))
        data["æ˜¯å¦åŒ…å«è¡¨æƒ…"].append(has_emoji)
        data["å¹³å°"].append(np.random.choice(platforms))
    
    return pd.DataFrame(data)

# åˆ†è¯å’Œè¯é¢‘ç»Ÿè®¡
def analyze_word_frequency(df, stop_words=None):
    """åˆ†æè¯„è®ºä¸­çš„è¯é¢‘ï¼Œè¿”å›è¯é¢‘ç»Ÿè®¡ç»“æœ"""
    if stop_words is None:
        # å¸¸è§åœç”¨è¯
        stop_words = set(["çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", 
                        "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", 
                        "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£", "è¿™ä¸ª", "é‚£ä¸ª", "å•Š",
                        "å§", "æŠŠ", "è¢«", "ç»™", "è®©", "ä»", "ä½†", "ä½†æ˜¯", "å¹¶", "ä¸ª", "å‘¢",
                        "å‘€", "å“¦", "å–”", "å—¯", "è¿™æ ·", "é‚£æ ·", "åª", "åªæœ‰", "å¯", "å¯ä»¥",
                        "æˆ–", "å¦‚æœ", "æ‰€ä»¥", "ä»€ä¹ˆ", "è¿™ä¹ˆ", "é‚£ä¹ˆ", "ä¸º", "ä¸ºäº†", "ä¹ˆ", "è¿˜"])
    
    all_words = []
    
    # éå†æ‰€æœ‰è¯„è®º
    for comment in df["è¯„è®º"]:
        # ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œæ ‡ç‚¹ç¬¦å·
        clean_comment = re.sub(r'[^\u4e00-\u9fa5]', '', comment)
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(clean_comment)
        # è¿‡æ»¤åœç”¨è¯
        filtered_words = [word for word in words if word not in stop_words and len(word) > 1]
        all_words.extend(filtered_words)
    
    # ç»Ÿè®¡è¯é¢‘
    word_counts = Counter(all_words)
    
    # è½¬æ¢ä¸ºDataFrame
    word_freq_df = pd.DataFrame({
        "è¯è¯­": list(word_counts.keys()),
        "é¢‘ç‡": list(word_counts.values())
    })
    
    # æŒ‰é¢‘ç‡æ’åº
    word_freq_df = word_freq_df.sort_values("é¢‘ç‡", ascending=False).reset_index(drop=True)
    
    return word_freq_df

# ä¸»å‡½æ•°
def main():
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    print("æ­£åœ¨ç”Ÿæˆç¤¾äº¤åª’ä½“è¯„è®ºæ•°æ®...")
    df = create_social_media_data(500)
    print(f"æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå…± {len(df)} æ¡è¯„è®º")
    
    # åˆ†è¯å¹¶ç»Ÿè®¡è¯é¢‘
    print("æ­£åœ¨è¿›è¡Œåˆ†è¯å’Œè¯é¢‘ç»Ÿè®¡...")
    word_freq_df = analyze_word_frequency(df)
    print(f"è¯é¢‘ç»Ÿè®¡å®Œæˆï¼Œå…± {len(word_freq_df)} ä¸ªæœ‰æ•ˆè¯è¯­")
    
    # åˆå§‹åŒ–PandasAI
    pandas_ai = pai.PandasAI()
    
    # 1. åŸºæœ¬ç»Ÿè®¡åˆ†æ
    print("\nåŸºæœ¬ç»Ÿè®¡åˆ†æ:")
    response = pandas_ai.run(
        df,
        "å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡ŒåŸºæœ¬ç»Ÿè®¡åˆ†æï¼ŒåŒ…æ‹¬å„å“ç‰Œçš„è¯„è®ºæ•°é‡ã€å„ä¸»é¢˜çš„è¯„è®ºåˆ†å¸ƒã€æƒ…æ„Ÿåˆ†å¸ƒæƒ…å†µç­‰ï¼Œå¹¶ç”¨å›¾è¡¨ç›´è§‚å±•ç¤º"
    )
    print(response)
    
    # 2. æƒ…æ„Ÿåˆ†æ
    print("\nå“ç‰Œæƒ…æ„Ÿåˆ†æ:")
    response = pandas_ai.run(
        df,
        "åˆ†æä¸åŒå“ç‰Œçš„æƒ…æ„Ÿè¯„åˆ†å¯¹æ¯”ï¼Œå“ªäº›å“ç‰Œè¯„ä»·æœ€å¥½ï¼Ÿå“ªäº›å“ç‰Œè¯„ä»·æœ€å·®ï¼Ÿè¯·ç»™å‡ºæ•°æ®æ”¯æŒçš„ç»“è®ºå¹¶ç”¨å›¾è¡¨å±•ç¤º"
    )
    print(response)
    
    # 3. æ—¶é—´è¶‹åŠ¿åˆ†æ
    print("\næ—¶é—´è¶‹åŠ¿åˆ†æ:")
    response = pandas_ai.run(
        df,
        "åˆ†æè¯„è®ºæƒ…æ„Ÿéšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿ï¼Œæ˜¯å¦æœ‰æ˜æ˜¾çš„å­£èŠ‚æ€§æ¨¡å¼æˆ–é‡å¤§äº‹ä»¶å½±å“ï¼Ÿå±•ç¤ºä¸€ä¸ªæŒ‰æœˆçš„æƒ…æ„Ÿå˜åŒ–æ›²çº¿å›¾"
    )
    print(response)
    
    # 4. è¯é¢‘åˆ†æ
    print("\nçƒ­é—¨è¯æ±‡åˆ†æ:")
    response = pandas_ai.run(
        word_freq_df,
        "åˆ†ææœ€å¸¸è§çš„20ä¸ªè¯è¯­ï¼Œè¿™äº›è¯è¯­åæ˜ äº†ç”¨æˆ·å¯¹äº§å“çš„å“ªäº›æ–¹é¢æœ€å…³æ³¨ï¼Ÿç”Ÿæˆä¸€ä¸ªè¯é¢‘æŸ±çŠ¶å›¾"
    )
    print(response)
    
    # 5. å¹³å°æ¯”è¾ƒåˆ†æ
    print("\nå¹³å°æ¯”è¾ƒåˆ†æ:")
    response = pandas_ai.run(
        df,
        "æ¯”è¾ƒä¸åŒå¹³å°ä¸Šçš„è¯„è®ºç‰¹ç‚¹ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå€¾å‘ã€è¯„è®ºé•¿åº¦ã€ç‚¹èµæ•°ç­‰æ–¹é¢ï¼Œä¸åŒå¹³å°çš„ç”¨æˆ·è¡¨ç°å‡ºå“ªäº›å·®å¼‚ï¼Ÿ"
    )
    print(response)
    
    # 6. ä¸»é¢˜ä¸æƒ…æ„Ÿäº¤å‰åˆ†æ
    print("\nä¸»é¢˜æƒ…æ„Ÿåˆ†æ:")
    response = pandas_ai.run(
        df,
        "åˆ†æä¸åŒä¸»é¢˜çš„äº§å“è·å¾—çš„æƒ…æ„Ÿè¯„ä»·ï¼Œå“ªç±»äº§å“è¯„ä»·æœ€é«˜ï¼Ÿå“ªç±»æœ€ä½ï¼Ÿå¯èƒ½çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
    )
    print(response)
    
    # 7. è¡¨æƒ…ç¬¦å·å½±å“åˆ†æ
    print("\nè¡¨æƒ…ç¬¦å·å½±å“åˆ†æ:")
    response = pandas_ai.run(
        df,
        "åˆ†æåŒ…å«è¡¨æƒ…ç¬¦å·çš„è¯„è®ºä¸ä¸åŒ…å«è¡¨æƒ…ç¬¦å·çš„è¯„è®ºåœ¨æƒ…æ„Ÿåˆ†æ•°ã€ç‚¹èµæ•°ç­‰æ–¹é¢çš„å·®å¼‚ï¼Œè¡¨æƒ…ç¬¦å·çš„ä½¿ç”¨æ˜¯å¦ä¸ç‰¹å®šæƒ…æ„Ÿç›¸å…³ï¼Ÿ"
    )
    print(response)
    
    # 8. ç‚¹èµæ•°ä¸æƒ…æ„Ÿå…³ç³»
    print("\nç‚¹èµæ•°ä¸æƒ…æ„Ÿå…³ç³»:")
    response = pandas_ai.run(
        df,
        "åˆ†æè¯„è®ºçš„ç‚¹èµæ•°ä¸æƒ…æ„Ÿåˆ†æ•°ä¹‹é—´çš„å…³ç³»ï¼Œæ­£é¢è¯„è®ºæ˜¯å¦è·å¾—æ›´å¤šç‚¹èµï¼Ÿè´Ÿé¢è¯„è®ºæ˜¯å¦ä¹Ÿèƒ½è·å¾—é«˜ç‚¹èµï¼Ÿ"
    )
    print(response)
    
    # 9. å“ç‰Œå£°èª‰åˆ†æ
    print("\nå“ç‰Œå£°èª‰ç»¼åˆåˆ†æ:")
    response = pandas_ai.run(
        df,
        "åŸºäºæƒ…æ„Ÿåˆ†æ•°ã€ç‚¹èµæ•°å’Œè¯„è®ºæ•°é‡ï¼Œç»¼åˆè¯„ä¼°å„å“ç‰Œçš„ç¤¾äº¤åª’ä½“å£°èª‰ï¼Œå¹¶å¯¹å„å“ç‰Œçš„ç¤¾äº¤åª’ä½“è¡¨ç°è¿›è¡Œæ’å"
    )
    print(response)
    
    # 10. æ¶ˆè´¹è€…æ´å¯ŸæŠ¥å‘Š
    print("\næ¶ˆè´¹è€…æ´å¯ŸæŠ¥å‘Š:")
    response = pandas_ai.run(
        [df, word_freq_df],
        "ç”Ÿæˆä¸€ä»½å…¨é¢çš„æ¶ˆè´¹è€…æ´å¯ŸæŠ¥å‘Šï¼ŒåŒ…æ‹¬å¸‚åœºè¶‹åŠ¿ã€å“ç‰Œè¡¨ç°ã€ç”¨æˆ·å…³æ³¨ç‚¹å’Œæ”¹è¿›å»ºè®®ç­‰å†…å®¹"
    )
    print(response)
    
    # å¯¼å‡ºæ•°æ®ä¾›åç»­ä½¿ç”¨
    df.to_csv("exports/social_media_comments.csv", index=False, encoding="utf-8-sig")
    word_freq_df.to_csv("exports/word_frequency.csv", index=False, encoding="utf-8-sig")

if __name__ == "__main__":
    main()
    print("åˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨å’Œåˆ†æç»“æœã€‚") 